# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.  
# SPDX-License-Identifier: CC-BY-NC-4.0
import json
from dataclasses import asdict
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

import pandas as pd
from loguru import logger

from poly_bench_evaluation.polybench_data import (
    AggregateOutput,
    PolyBenchOutput,
    PolyBenchRetrievalMetrics,
)


def instance_level_scoring(
    instance_id: str,
    result: Dict,
    f2p: List[str],
    p2p: List[str],
    patch_applied: bool,
    generation: bool,
) -> Union[PolyBenchOutput, PolyBenchRetrievalMetrics]:
    """Logging and storing function (instance level).

    This function returns a PolyBenchOutput json.

    Args:
        instance_id: instance id
        result: parsed results of the instance
        f2p: list of f2p tests
        p2p: list of p2p tests
        patch_applied: whether patch is applied or not
        generation: whether a patch is generated or not
    """
    with_logs = False
    all_f2p_passed = False
    no_p2p_failed = False
    resolved = False
    passed_tests = []
    failed_tests = []

    if result:
        with_logs = True
        passed_tests = result["passed_tests"]
        failed_tests = result["failed_tests"]

        passed_tests_set = set(result["passed_tests"])
        failed_tests_set = set(result["failed_tests"])

        f2p_set = set(f2p)
        p2p_set = set(p2p)

        if f2p_set.intersection(passed_tests_set) == f2p_set:
            all_f2p_passed = True

        if len(p2p_set.intersection(failed_tests_set)) == 0:
            no_p2p_failed = True

        if all_f2p_passed and no_p2p_failed:
            resolved = True

    output = PolyBenchOutput(
        instance_id=instance_id,
        patch_applied=patch_applied,
        generation=generation,
        with_logs=with_logs,
        all_f2p_passed=all_f2p_passed,
        no_p2p_failed=no_p2p_failed,
        resolved=resolved,
        passed_tests=passed_tests,
        failed_tests=failed_tests,
    )

    return output


def store_instance_level_output(
    instance_output: Union[PolyBenchOutput, PolyBenchRetrievalMetrics],
    result_path: str,
    suffix: str = "_result",
):
    """Store the instance level output in a json file.

    Args:
        instance_output: instance level output
        result_path: path to store the result
    """
    output_path = Path(result_path)
    output_path.mkdir(exist_ok=True)

    # Check if instance_output is of the right class
    if not isinstance(instance_output, PolyBenchOutput) and not isinstance(
        instance_output, PolyBenchRetrievalMetrics
    ):
        raise TypeError(
            "instance_output must be of type PolyBenchOutput or PolyBenchRetrievalMetrics"
        )

    with open(output_path / f"{instance_output.instance_id}{suffix}.json", "w") as f:
        json.dump(asdict(instance_output), f, indent=4)


def _get_all_instance_results(
    result_path: Union[str, Path], suffix: str = "_result"
) -> List[Dict[str, Any]]:
    """
    Get all instance results from JSON files in the specified directory.

    Args:
        result_path: Directory path containing result JSON files

    Returns:
        List of dictionaries containing instance results

    Raises:
        TypeError: If result_path is not str or Path
        FileNotFoundError: If directory doesn't exist
        PermissionError: If files can't be accessed
        json.JSONDecodeError: If JSON files are invalid
    """
    if not isinstance(result_path, (str, Path)):
        raise TypeError(f"Expected str or Path, got {type(result_path)}")

    result_path = Path(result_path)
    if not result_path.is_dir():
        raise FileNotFoundError(f"Directory not found: {result_path}")

    all_instance_results = []
    for file in result_path.glob(f"*{suffix}.json"):
        with open(file, "r") as f:
            all_instance_results.append(json.load(f))
    if len(all_instance_results) == 0:
        raise ValueError(f"No results found in {str(result_path)}")

    return all_instance_results


def _flatten_retrieval_list(retrieval_list: List[Dict[str, Any]], prefix: str):
    return [
        {
            "instance_id": item["instance_id"],
            f"{prefix}_recall": item["metrics"]["recall"],
            f"{prefix}_precision": item["metrics"]["precision"],
            f"{prefix}_f1": item["metrics"]["f1"],
        }
        for item in retrieval_list
    ]


def _get_retrieval_metrics_for_agg(
    result_path: str,
) -> Tuple[
    List[Dict[str, float]], Optional[List[Dict[str, float]]], pd.DataFrame, Optional[pd.DataFrame]
]:
    # Compute retrieval metrics
    all_metrics_jsons = _get_all_instance_results(result_path, suffix="_metrics")
    file_retrieval_results = [
        {"instance_id": metrics["instance_id"], "metrics": metrics["file_retrieval_metrics"]}
        for metrics in all_metrics_jsons
    ]
    file_retrieval_results = _flatten_retrieval_list(file_retrieval_results, prefix="file")
    node_retrieval_results = [
        {"instance_id": metrics["instance_id"], "metrics": metrics["node_retrieval_metrics"]}
        for metrics in all_metrics_jsons
        if "node_retrieval_metrics" in metrics and metrics["node_retrieval_metrics"] is not None
    ]
    node_retrieval_results = _flatten_retrieval_list(node_retrieval_results, prefix="node")

    # Create DataFrame to compute file retrieval metrics
    file_re_df = pd.DataFrame(file_retrieval_results)

    # Create DataFrame to compute node retrieval metrics
    node_re_df = None
    if node_retrieval_results:
        node_re_df = pd.DataFrame(node_retrieval_results)
        node_re_df.dropna(inplace=True)

        # There are 3 instances where patch application of GT patch fails
        # which results in recall scores of -1. Need to fix but filter them
        # out for now.
        node_re_df = node_re_df.query("node_recall >= 0.0")

    return (file_retrieval_results, node_retrieval_results, file_re_df, node_re_df)


def aggregate_logs(
    result_path: str, dataset_path: str, output_path: str = "./", metrics_only: bool = False
):
    """Aggregate all the logs into a single json file.
    Args:
        result_path: The directory where all instance level json files are stored
        dataset_path: The polybench dataset path
        output_path: The path where aggregated results are stored (default: current working directory)
        metrics_only: Whether to only aggregate and store metrics
    """

    result = AggregateOutput(
        resolved=[],
        patch_applied=[],
        with_logs=[],
        generation=[],
        no_generation=[],
        not_resolved=[],
        total_empty_patch_instances=0,
        total_instances=0,
        total_resolved=0,
        total_unresolved=0,
        file_retrieval=[{}],
        node_retrieval=None,
    )

    if not metrics_only:
        all_jsons = _get_all_instance_results(result_path)
        for data in all_jsons:
            if data.get("resolved"):
                result.resolved.append(data["instance_id"])

            if not data.get("resolved"):
                result.not_resolved.append(data["instance_id"])

            if data.get("patch_applied"):
                result.patch_applied.append(data["instance_id"])

            if data.get("with_logs"):
                result.with_logs.append(data["instance_id"])

            if data.get("generation"):
                result.generation.append(data["instance_id"])

            if not data.get("generation"):
                result.no_generation.append(data["instance_id"])

            if not data.get("generation") and not data.get("patch_applied"):
                result.total_empty_patch_instances += 1

        result.total_resolved = len(result.resolved)
        result.total_unresolved = len(result.not_resolved)
        result.total_instances = result.total_resolved + result.total_unresolved

    file_ret_metrics, node_ret_metrics, file_re_df, node_re_df = _get_retrieval_metrics_for_agg(
        result_path
    )
    result.file_retrieval = file_ret_metrics
    result.node_retrieval = node_ret_metrics

    if metrics_only:
        file_name = "metrics.json"
    else:
        file_name = "result.json"

    logger.info(f"Writing aggregated results to {Path(output_path) / file_name}")
    with open(f"{Path(output_path) / file_name}", "w") as f:
        json.dump(asdict(result), f, indent=4)

    # Print results
    dataset = pd.read_csv(dataset_path)
    if not metrics_only:
        _print_detailed_results(dataset, result)
    _print_retrieval_metrics(dataset, file_re_df, node_re_df)


def _print_retrieval_metrics(
    dataset: pd.DataFrame,
    file_retrieval_df: pd.DataFrame,
    node_retrieval_df: pd.DataFrame,
) -> None:
    # Print file retrieval by language
    print("\nFile retrieval metrics by language:")
    merged = dataset.merge(file_retrieval_df, on="instance_id")
    print(
        merged[["language", "file_recall", "file_precision", "file_f1"]]
        .groupby("language")
        .mean()
        .round(2)
        .to_string()
    )

    print("\nFile retrieval metrics overall:")
    print(
        file_retrieval_df[["file_recall", "file_precision", "file_f1"]].mean().round(2).to_string()
    )

    # Print node retrieval by language
    if node_retrieval_df is not None:
        print("\nNode retrieval metrics by language:")
        merged = dataset.merge(node_retrieval_df, on="instance_id")
        print(
            merged[["language", "node_recall", "node_precision", "node_f1"]]
            .groupby("language")
            .mean()
            .round(2)
            .to_string()
        )

        print("\nNode retrieval metrics overall:")
        print(
            node_retrieval_df[["node_recall", "node_precision", "node_f1"]]
            .mean()
            .round(2)
            .to_string()
        )


def _print_detailed_results(
    dataset: pd.DataFrame,
    result: AggregateOutput,
) -> None:

    assert "language" in dataset.columns, "language column not found in dataset file."
    assert "instance_id" in dataset.columns, "instance_id column not found in dataset file."
    assert "task_category" in dataset.columns, "task_category column not found in dataset file."

    resolved_ids = set(result.resolved)  # noqa: F841

    print(f"Total resolved: {result.total_resolved}/{result.total_instances}")
    if result.total_instances > 0:
        print(f"Total pass rate: {(len(result.resolved) / result.total_instances):.2%}\n")

    # Print pass rates by language
    print("Pass rate by language:")
    for lang, total in dataset.language.value_counts().to_dict().items():
        resolved_lang = len(dataset.query("language == @lang & instance_id in @resolved_ids"))
        rate = resolved_lang / total
        print(f"{lang}: {resolved_lang}/{total} ({rate:.1%})")

    # Print pass rates by task category
    print("\nPass rate by task category:")
    for category, total in dataset.task_category.value_counts().to_dict().items():
        resolved_category = len(
            dataset.query("task_category == @category & instance_id in @resolved_ids")
        )
        rate = resolved_category / total
        print(f"{category}: {resolved_category}/{total} ({rate:.1%})")
